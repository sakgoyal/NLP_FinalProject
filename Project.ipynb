{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U npc-gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from npc_gzip.compressors.gzip_compressor import GZipCompressor\n",
    "from npc_gzip.knn_classifier import KnnClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.set_option('display.width', 1000)  # For better display of dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "file = './yelp_academic_dataset_review'\n",
    "try: os.remove(f\"{file}.csv\")\n",
    "except: pass\n",
    "\n",
    "# Read the first 1 million rows\n",
    "df = pd.read_json(f'{file}.json', lines=True, nrows=1_000_000)\n",
    "\n",
    "# Remove all rows where the text has less than 100 characters to ensure high quality reviews\n",
    "df = df[df['text'].str.len() > 100]\n",
    "\n",
    "# remove unnecessary columns to save space and time\n",
    "df.drop(['review_id', 'date', 'user_id', 'business_id'], axis=1, inplace=True)\n",
    "\n",
    "# map [0,5] stars to negative (0), neutral (1), positive (2)\n",
    "df['sentiment'] = df['stars'].map({0 : 0, 1 : 0, 2 : 0, 3 : 1, 4 : 2, 5 : 2})\n",
    "\n",
    "# Remove all newlines and carriage returns from the text\n",
    "df.replace('\\n', ' ', regex=True, inplace=True)\n",
    "df.replace('\\r', ' ', regex=True, inplace=True)\n",
    "df.replace('  ', ' ', regex=True, inplace=True)\n",
    "\n",
    "# Duplicate rows based on the 'useful' voted reviews to bias the model towards helpful reviews\n",
    "useful_duplicated = pd.DataFrame(df.reindex(df.index.repeat(df['useful'] + 1)).reset_index(drop=True))\n",
    "useful_duplicated.to_csv(f'{file}.csv', header=True, index=False, mode='w')\n",
    "useful_duplicated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pd.read_csv(f'{file}.csv')))\n",
    "pd.read_csv(f'{file}.csv', nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "\n",
    "def get_data(n_samples: int):\n",
    "    \"\"\"\n",
    "    Pulls the Yelp dataset from the local file system\n",
    "    and returns the training and test datasetsas tuples.\n",
    "    Each contains the text and labels as np arrays.\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f'{file}.csv', nrows=n_samples)\n",
    "    print('number of samples loaded', len(df))\n",
    "    text, stars = df['text'], df['sentiment']\n",
    "    split : Sequence[pd.DataFrame] = train_test_split(text, stars, test_size=0.2, random_state=1) #, stratify=stars)\n",
    "    text_train, text_test, stars_train, stars_test = split\n",
    "        \n",
    "\n",
    "    train = text_train.to_numpy(), stars_train.to_numpy()\n",
    "    test = text_test.to_numpy(), stars_test.to_numpy()\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def fit_model(train_text: np.ndarray, train_labels: np.ndarray, distance_metric: str = \"ncd\") -> KnnClassifier:\n",
    "    \"\"\" Fits a Knn-GZip compressor on the train data and returns it.\n",
    "    Arguments:\n",
    "        train_text (np.ndarray): Training dataset as a numpy array.\n",
    "        train_labels (np.ndarray): Training labels as a numpy array.\n",
    "\n",
    "    Returns: KnnClassifier: Trained Knn-Compressor model ready to make predictions.\n",
    "    \"\"\"\n",
    "    return KnnClassifier(\n",
    "        compressor=GZipCompressor(), \n",
    "        distance_metric=distance_metric,\n",
    "        training_inputs=train_text,\n",
    "        training_labels=train_labels,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fetching data...\")\n",
    "(train_text, train_labels), (test_text, test_labels) = get_data(n_samples=1_00000)\n",
    "\n",
    "print(\"Fitting model...\")\n",
    "model = fit_model(train_text, train_labels)\n",
    "\n",
    "random_indicies = np.random.choice(test_text.shape[0], len(test_labels), replace=False)\n",
    "print(random_indicies.shape)\n",
    "sample_test_text = test_text[random_indicies]\n",
    "sample_test_labels = test_labels[random_indicies]\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "\"\"\" Here we use the `sampling_percentage` to save time at the expense of worse predictions. \n",
    "This `sampling_percentage` selects a random % of training data to compare `sample_test_text` \n",
    "against rather than comparing it against the entire training dataset. \"\"\"\n",
    "(distances, labels, similar_samples) = model.predict( sample_test_text, top_k=1, sampling_percentage=0.1 )\n",
    "\n",
    "print(\"Final Accuracy = \", accuracy_score(sample_test_labels, labels.reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the state of the KNN model to a file\n",
    "import pickle\n",
    "\n",
    "program_state = {\n",
    "    'random_indicies': random_indicies,\n",
    "    'distances': distances,\n",
    "    'labels': labels,\n",
    "    'sample_test_labels' : sample_test_labels,\n",
    "}\n",
    "\n",
    "def save_state(filename, data):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "def load_state(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Save the state to a file\n",
    "save_state('program_state.pkl', program_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the state from the file\n",
    "loaded_state = load_state('program_state.pkl')\n",
    "\n",
    "random_indicies    = loaded_state['random_indicies']\n",
    "distances          = loaded_state['distances']\n",
    "labels             = loaded_state['labels']\n",
    "sample_test_labels = loaded_state['sample_test_labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fetching data...\")\n",
    "(train_text, train_labels), (test_text, test_labels) = get_data(n_samples=1_00000)\n",
    "\n",
    "print(\"Fitting model...\")\n",
    "model = fit_model(train_text, train_labels)\n",
    "\n",
    "print(random_indicies.shape)\n",
    "sample_test_text = test_text[random_indicies]\n",
    "sample_test_labels = test_labels[random_indicies]\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "\"\"\" Here we use the `sampling_percentage` to save time at the expense of worse predictions. \n",
    "This `sampling_percentage` selects a random % of training data to compare `sample_test_text` \n",
    "against rather than comparing it against the entire training dataset. \"\"\"\n",
    "(distances, labels, similar_samples) = model.predict( sample_test_text, top_k=1, sampling_percentage=0.1 )\n",
    "\n",
    "print(\"Final Accuracy = \", accuracy_score(sample_test_labels, labels.reshape(-1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
